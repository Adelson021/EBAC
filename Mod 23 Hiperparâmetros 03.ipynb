{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adf18a6b-6732-4bb1-be50-b2c2ac0fd525",
   "metadata": {},
   "source": [
    "O Random Forest (RF) possui vários hiperparâmetros que permitem ajustar o desempenho e o comportamento do modelo para melhor se adequar aos dados. Abaixo estão os principais hiperparâmetros e a explicação de como eles afetam o modelo:\n",
    "\n",
    "**1. Principais Hiperparâmetros do Random Forest**\n",
    "\n",
    "* a) n_estimators\n",
    "\n",
    "Descrição: Número de árvores na floresta.  \n",
    "Função: Controla quantas árvores de decisão serão treinadas e, posteriormente, agregadas para fazer a previsão. Valores maiores geralmente aumentam a precisão, mas também aumentam o tempo de processamento. Este hiperparâmetro deve ser ajustado para equilibrar precisão e custo computacional.\n",
    "\n",
    "* b) max_features\n",
    "\n",
    "Descrição: Número máximo de variáveis a serem consideradas em cada divisão de uma árvore.  \n",
    "Função: Controla a quantidade de variabilidade entre as árvores, reduzindo a correlação entre elas e aumentando a robustez do modelo. Os valores mais comuns são:  \n",
    "\"sqrt\": a raiz quadrada do número total de variáveis.  \n",
    "\"log2\": o logaritmo de base 2 do número de variáveis.  \n",
    "None: usa todas as variáveis, como no Bagging.  \n",
    "Impacto: Diminuir max_features aumenta a diversidade entre as árvores, reduzindo o overfitting, mas pode impactar negativamente a precisão se for muito baixo.\n",
    "\n",
    "* c) max_depth\n",
    "\n",
    "Descrição: Profundidade máxima de cada árvore.  \n",
    "Função: Limita a quantidade de divisões que cada árvore pode fazer, controlando o quão “profundamente” a árvore pode se adaptar aos dados de treino. Limitar a profundidade das árvores ajuda a evitar o overfitting, especialmente em conjuntos de dados complexos e ruidosos.\n",
    "\n",
    "* d) min_samples_split\n",
    "\n",
    "Descrição: Número mínimo de amostras necessárias para dividir um nó.  \n",
    "Função: Controla o tamanho mínimo de dados que um nó deve ter antes de dividir em novos ramos. Valores mais altos restringem o crescimento das árvores e tornam o modelo mais generalizado, reduzindo o overfitting.\n",
    "\n",
    "* e) min_samples_leaf\n",
    "\n",
    "Descrição: Número mínimo de amostras necessárias em cada folha.  \n",
    "Função: Define o número mínimo de observações que uma folha (nó terminal) deve conter. Esse parâmetro ajuda a evitar folhas com muito poucos dados, o que pode tornar o modelo mais sensível ao ruído e aumentar o overfitting.\n",
    "\n",
    "* f) bootstrap\n",
    "\n",
    "Descrição: Se as amostras de treino serão geradas com reposição (bootstrap).  \n",
    "Função: Controla se cada árvore será treinada em uma amostra bootstrap (amostra aleatória com reposição) do conjunto de dados original. Quando bootstrap=True, cada árvore será treinada com uma amostra diferente dos dados, aumentando a variabilidade e a robustez do modelo.\n",
    "\n",
    "* g) oob_score\n",
    "\n",
    "Descrição: Indica se deve ser usada a pontuação fora-da-amostra (Out-Of-Bag).  \n",
    "Função: Quando oob_score=True, o modelo avalia seu desempenho usando as amostras que não foram selecionadas nas amostras bootstrap para treino (Out-Of-Bag). É útil para validação cruzada e avaliação interna do desempenho sem a necessidade de um conjunto de teste separado.\n",
    "\n",
    "* h) n_jobs\n",
    "\n",
    "Descrição: Número de processadores a serem usados em paralelo.  \n",
    "Função: Define quantos núcleos da CPU o algoritmo utilizará durante o treinamento e a previsão. Valores como -1 usam todos os núcleos disponíveis, enquanto valores positivos específicos indicam um número específico de núcleos. Esse parâmetro pode reduzir bastante o tempo de treinamento em grandes conjuntos de dados.\n",
    "\n",
    "* i) random_state\n",
    "\n",
    "Descrição: Define uma semente para a geração de números aleatórios.  \n",
    "Função: Garante a reprodutibilidade dos resultados, pois configura uma semente para as amostras aleatórias geradas durante o treinamento (como amostras bootstrap e seleção de variáveis). Útil para análise consistente e para reproduzir os mesmos resultados em experimentos repetidos.\n",
    "\n",
    "* j) class_weight\n",
    "\n",
    "Descrição: Peso das classes para ajuste de amostras desequilibradas.  \n",
    "Função: Define pesos para as classes para lidar com desequilíbrios de classes em problemas de classificação. O parâmetro \"balanced\" ajusta automaticamente o peso para equilibrar as classes, com maior peso para a classe minoritária, melhorando a precisão para dados desbalanceados.\n",
    "\n",
    "\n",
    "**Resumo e Ajuste dos Hiperparâmetros**\n",
    "\n",
    "Os hiperparâmetros do Random Forest controlam a complexidade e o desempenho do modelo, influenciando:\n",
    "\n",
    "Precisão: n_estimators, max_features e bootstrap.  \n",
    "Complexidade e Generalização: max_depth, min_samples_split, min_samples_leaf.  \n",
    "Eficiência Computacional: n_jobs.  \n",
    "Robustez para Classes Desbalanceadas: class_weight.  \n",
    "\n",
    "Esses parâmetros devem ser ajustados conforme o tipo de problema, o volume e a complexidade dos dados, balanceando precisão, generalização e tempo de processamento para alcançar o melhor desempenho do modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e9dda0-6628-4b2c-be80-b6a4dfd2f6ac",
   "metadata": {},
   "source": [
    "Um exemplo de implementação do Random Forest em Python usando a biblioteca scikit-learn e explorando alguns dos hiperparâmetros com a fórmula de ajuste de modelo pode ser ilustrado com um conjunto de dados de exemplo. Vamos utilizar o dataset iris, um conjunto de dados clássico para classificação, e ajustar os hiperparâmetros do Random Forest para ilustrar o impacto de n_estimators, max_depth, e max_features.\n",
    "\n",
    "Aqui está o código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e37c01db-4a36-4194-985a-842db16778c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia do Random Forest: 1.00\n"
     ]
    }
   ],
   "source": [
    "# Importando as bibliotecas necessárias\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Carregar o dataset iris\n",
    "data = load_iris()\n",
    "X = data.data  # Variáveis independentes\n",
    "y = data.target  # Variável dependente\n",
    "\n",
    "# Dividir os dados em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Criar o modelo Random Forest com parâmetros ajustados\n",
    "modelo_rf = RandomForestClassifier(\n",
    "    n_estimators=100,      # Número de árvores\n",
    "    max_depth=4,           # Profundidade máxima das árvores\n",
    "    max_features=\"sqrt\",   # Número máximo de variáveis consideradas em cada divisão\n",
    "    random_state=42        # Semente para resultados reprodutíveis\n",
    ")\n",
    "\n",
    "# Treinar o modelo\n",
    "modelo_rf.fit(X_train, y_train)\n",
    "\n",
    "# Fazer previsões no conjunto de teste\n",
    "y_pred = modelo_rf.predict(X_test)\n",
    "\n",
    "# Calcular e exibir a acurácia do modelo\n",
    "acuracia = accuracy_score(y_test, y_pred)\n",
    "print(f\"Acurácia do Random Forest: {acuracia:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96404833-9cb5-41ff-8831-ad667638fd8c",
   "metadata": {},
   "source": [
    "**Explicação dos Parâmetros Utilizados**\n",
    "\n",
    "n_estimators=100: Esse parâmetro define o número de árvores no modelo. Neste exemplo, usamos 100 árvores, o que é um valor comum para boa precisão.  \n",
    "max_depth=4: Limita a profundidade máxima de cada árvore para 4, o que ajuda a evitar que o modelo fique excessivamente complexo e tenha problemas de overfitting.  \n",
    "max_features=\"sqrt\": Define que a raiz quadrada do número total de variáveis será considerada em cada divisão. Isso torna cada árvore mais diversa e reduz a correlação entre elas.  \n",
    "random_state=42: Define uma semente para garantir a reprodutibilidade dos resultados.  \n",
    "\n",
    "**Resultados**  \n",
    "Após o treinamento, o modelo faz previsões e calcula a acurácia com accuracy_score. Esse valor indica a porcentagem de previsões corretas no conjunto de teste, oferecendo uma medida de desempenho para o modelo.\n",
    "\n",
    "Esse exemplo mostra um ajuste básico de hiperparâmetros no Random Forest, que você pode otimizar com validação cruzada ou ajuste de hiperparâmetros automatizados (usando, por exemplo, GridSearchCV)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaeec9b9-08a1-448f-bf72-a1b0c9266cb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
